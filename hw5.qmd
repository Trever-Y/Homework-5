---
title: "Model Comparison"
author: "Trever Yoder"
format: pdf
editor: visual
---

## Task 1: Conceptual Questions

-   **What is the purpose of using cross-validation when fitting a random forest model?**

    Cross‑validation is used to estimate how well the random forest will perform on unseen data by repeatedly splitting the data into “folds,” training on some folds and validating on the others. This helps guard against overfitting and provides a more reliable measure of out‑of‑sample predictive accuracy.

-   **Describe the bagged tree algorithm.**

    Bagged trees (bootstrap aggregating) build many decision trees on different bootstrap samples of the training data and then average (for regression) or majority‑vote (for classification) their predictions. By aggregating across models trained on varied subsets, bagging reduces variance and improves stability compared to a single decision tree.

-   **What is meant by a general linear model?**

    A general linear model (GLM) expresses a continuous response variable (Y) as a linear combination of predictors where epsilon is assumed to be normally distributed with constant variance. It encompasses methods like ANOVA, ANCOVA, and multiple linear regression.

-   **When fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?**

    An interaction term allows the effect of one predictor on the response to change depending on the level of the other predictor. Without interactions the model assumes additive effects; with interactions it can capture non‑additive relationships, where the slope for (X_1) varies by the value of (X_2).

-   **Why do we split our data into a training and test set?**

    Splitting data ensures that we can train the model on one subset (training set) and then evaluate its performance on completely unseen data (test set). This provides an unbiased assessment of how well the model generalizes to new observations and helps detect overfitting.

## Task 2: Data Prep

### Packages and Data

```{r load-packages, message = FALSE, warning = FALSE}
library(tidyverse)
library(tidymodels)
library(caret)
library(yardstick)
library(glmnet)

Heart_data <- read_csv(
  "heart.csv") 
summary(Heart_data)
```

### Question 1: Variable Type

Heart Disease is treated as a quantitative variable which does not make sense. Conceptually, Heart Disease is a categorical variable since 0 means no heart disease and 1 means presence of heart disease. A decimal like 0.5 wouldn't fall into either category. A person either has or does not have heart disease (1 or 0).

### Question 2: Create New Data

```{r}
new_heart <- Heart_data %>%
  mutate(Heart_Disease = factor(HeartDisease, levels = c(0, 1), labels = c("No", "Yes"))) %>%
  select(-ST_Slope, -HeartDisease)
```

## Task 3: EDA

### Question 1: Plot to check for interaction

```{r}
ggplot(new_heart, aes(x = MaxHR, y = Age, color = Heart_Disease)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE) +
  scale_color_viridis_d(option = "D") +
  labs(
    title = "Age vs Max Heart Rate by Heart Disease Status",
    x = "Max Heart Rate",
    y = "Age",
    color = "Heart Disease"
  ) +
  theme_minimal()
```

### Question 2: Conclusion based on visual evidence

We can see that the slopes are different. An additive model would assume their slopes to be the same. Therefore, to account for different slopes, we need to use an interaction model.

## Task 4: Testing and Training

```{r}
set.seed(101)
split <- initial_split(new_heart, prop = 0.8)
train <- training(split)
test <- testing(split)
```

## Task 5: OLS and LASSO

### Question 1: Interaction model

```{r}
ols_mlr <- lm(Age ~ MaxHR * Heart_Disease, data = train)
summary(ols_mlr)
```

### Question 2: RMSE

```{r}
predictions <- predict(ols_mlr, newdata = test)
residuals <- test$Age - predictions
OLS_rmse <- sqrt(mean(residuals^2))
OLS_rmse
```

### Question 3: Performance

```{r}
LASSO_recipe <- recipe(Age ~ MaxHR + Heart_Disease, data = train) %>%
  step_dummy(Heart_Disease) %>%
  step_normalize(all_predictors()) %>%       # Standardize all predictors
  step_interact( ~ MaxHR:starts_with("Heart_Disease_") )  # Add interaction terms
LASSO_recipe
```

### Question 4: Spec, Grid, Results

```{r}
LASSO_spec <- linear_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet")
cv_folds <- vfold_cv(train, v = 10)

LASSO_wf <- workflow() %>%
  add_model(LASSO_spec) %>%
  add_recipe(LASSO_recipe)

lasso_grid <- grid_regular(penalty(), levels = 200)

LASSO_results <- tune_grid(
  LASSO_wf,
  resamples = cv_folds,
  grid = lasso_grid,
  metrics = metric_set(rmse)
)

best_LASSO <- select_best(LASSO_results, metric = "rmse")
best_LASSO

final_lasso_wf <- finalize_workflow(LASSO_wf, best_LASSO)
final_lasso_fit <- fit(final_lasso_wf, data = train)
tidy(final_lasso_fit)
```

### Question 5: RMSE

I would expect the RMSE to be very similar since the LASSO applied very little penalty (1e-10) and the same variables were used. The scale is different (ie our intercept is only 54) since we normalized the variables, but that won't change the RMSE.

```{r}
# Get LASSO predictions
lasso_preds <- predict(final_lasso_fit, new_data = test) %>%
  bind_cols(test)

# Compute RMSE
rmse_lasso <- rmse(lasso_preds, truth = Age, estimate = .pred)
rmse_lasso
```

### Question 6: Compare RMSE

We can see that both models have similar RMSE.

```{r}
rmse_lasso
OLS_rmse
```

### Question 7: Why are they same even though we have different coefficient?

As stated, we standardized our variables in the LASSO model, which makes the coefficients look different. However, we use the same variables and interactions, so they are making very similar predictions. Thus, our RMSE is nearly the same.

## Task 6: Logistic Regression

### Question 1: Fit 2 models and Identify Best models

```{r}
ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3, classProbs = TRUE, summaryFunction = twoClassSummary)

# Model 1
logit_model1 <- train(
  Heart_Disease ~ MaxHR + Age + Sex + Cholesterol,
  data = train,
  method = "glm",
  family = "binomial",
  trControl = ctrl,
  metric = "ROC"
)

# Model 2
logit_model2 <- train(
  Heart_Disease ~ Age + MaxHR + Cholesterol + RestingBP + Sex + ChestPainType,
  data = train,
  method = "glm",
  family = "binomial",
  trControl = ctrl,
  metric = "ROC"
)

#ID best model
logit_model1$results
logit_model2$results

#provide summary of best model
summary(logit_model2$finalModel) 
```

Model 2 is our best model with an ROC of .848 (vs .771). The sensitivy and specificity standard deviation is also lower among folds for model 2, which indicates consistency. Model 2 also has higher sensitivity and specificity, which I will define in question 3.

### Question 2: Check on Test data with confusion matrix

```{r}
logit_preds <- predict(logit_model2, newdata = test)
conf_matrix <- confusionMatrix(logit_preds, test$Heart_Disease)
conf_matrix
```

### Question 3: ID Values of Sensitivity etc.

```{r}
conf_matrix$byClass["Sensitivity"]
conf_matrix$byClass["Specificity"]
```

Sensitivity: Proportion of people with heart disease that our model correctly identifies 

- Our Model can correctly identify 76.6% of people with heart disease (23.4% false negative)

## Specificity: proportion of people without heart disease that our model correctly identifies

- Our model can correctly identify 77.8% of people without heart disease (22.2% false positive)
