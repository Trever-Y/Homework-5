---
title: "Model Comparison"
author: "Trever Yoder"
format: pdf
editor: visual
---

## Task 1: Conceptual Questions

### Question 1:

-   **What is the purpose of using cross-validation when fitting a random forest model?**

    Cross‑validation is used to estimate how well the random forest will perform on unseen data by repeatedly splitting the data into “folds,” training on some folds and validating on the other(s). This helps guard against overfitting and provides a more reliable measure of out‑of‑sample predictive accuracy.
    
### Question 2:    

-   **Describe the bagged tree algorithm.**

    Bagged trees build many decision trees on different bootstrap samples of the training data and then average their predictions. By aggregating across models trained on varied subsets, bagging reduces variance and improves stability compared to a single decision tree.

### Question 3:

-   **What is meant by a general linear model?**

    A general linear model expresses a continuous response variable as a linear combination of predictors where the errors are assumed to be normally distributed with constant variance. It encompasses methods like ANOVA, ANCOVA, and multiple linear regression.

### Question 4:

-   **When fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?**

    An interaction term allows the effect of one predictor on the response to change depending on the level of the other predictor. Without interactions the model assumes additive effects, but with interactions, it can capture non‑additive relationships, where the slope for one variable varies by the value of another.

### Question 5:

-   **Why do we split our data into a training and test set?**

    Splitting data ensures that we can train the model on one subset (training set) and then evaluate its performance on completely unseen data (test set). This provides an unbiased assessment of how well the model generalizes to new observations and helps detect overfitting.

## Task 2: Data Prep

### Packages and Data

```{r load-packages, message = FALSE, warning = FALSE}
library(tidyverse)
library(tidymodels)
library(caret)
library(yardstick)
library(glmnet)

#Read in data and print summary
Heart_data <- read_csv(
  "heart.csv") 
summary(Heart_data)
```

### Question 1: Variable Type

Heart Disease is treated as a quantitative variable which does not make sense. Conceptually, Heart Disease is a categorical variable since 0 means no heart disease and 1 means presence of heart disease. A decimal like 0.5 wouldn't fall into either category. A person either has or does not have heart disease (1 or 0).

### Question 2: Create New Data

```{r}
#Create new heart data
new_heart <- Heart_data %>%
  mutate(Heart_Disease = factor(HeartDisease, levels = c(0, 1), labels = c("No", "Yes"))) %>%
  select(-ST_Slope, -HeartDisease)
```

## Task 3: EDA

### Question 1: Plot to check for interaction

```{r}
#plot age vs max heart rate by disease status
ggplot(new_heart, aes(x = MaxHR, y = Age, color = Heart_Disease)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE) +
  scale_color_brewer(palette = "Dark2") +
  labs(
    title = "Age vs Max Heart Rate by Heart Disease Status",
    x = "Max Heart Rate",
    y = "Age",
    color = "Heart Disease"
  ) +
  theme_minimal()
```

### Question 2: Conclusion based on visual evidence

We can see that the slopes are different. An additive model would assume their slopes to be the same. Therefore, to account for different slopes, we need to use an interaction model.

## Task 4: Testing and Training

```{r}
#set seed for repeat ability and split data into train and test data
set.seed(101)
split <- initial_split(new_heart, prop = 0.8)
train <- training(split)
test <- testing(split)
```

## Task 5: OLS and LASSO

### Question 1: Interaction model

```{r}
#specify model
ols_mlr <- lm(Age ~ MaxHR * Heart_Disease, data = train)
summary(ols_mlr)
```

### Question 2: RMSE

```{r}
#calculate RMSE
predictions <- predict(ols_mlr, newdata = test)
residuals <- test$Age - predictions
OLS_rmse <- sqrt(mean(residuals^2))
OLS_rmse
```

### Question 3: Performance

```{r}
#Create Recipe
LASSO_recipe <- recipe(Age ~ MaxHR + Heart_Disease, data = train) %>%
  step_dummy(Heart_Disease) %>%
  step_normalize(all_predictors()) %>% 
  step_interact( ~ MaxHR:starts_with("Heart_Disease_") ) 
LASSO_recipe
```

### Question 4: Spec, Grid, Results

```{r}
#Create specs and folds
LASSO_spec <- linear_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet")
cv_folds <- vfold_cv(train, v = 10)

#Create workflow
LASSO_wf <- workflow() %>%
  add_model(LASSO_spec) %>%
  add_recipe(LASSO_recipe)

#create grid
lasso_grid <- grid_regular(penalty(), levels = 200)

#tune
LASSO_results <- tune_grid(
  LASSO_wf,
  resamples = cv_folds,
  grid = lasso_grid,
  metrics = metric_set(rmse)
)

#print best lasso
best_LASSO <- select_best(LASSO_results, metric = "rmse")
best_LASSO

#setup and report results from final model
final_lasso_wf <- finalize_workflow(LASSO_wf, best_LASSO)
final_lasso_fit <- fit(final_lasso_wf, data = train)
tidy(final_lasso_fit)
```

### Question 5: RMSE

I would expect the RMSE to be very similar since the LASSO applied very little penalty (1e-10) and the same variables and interaction were used. The scale is different (ie our intercept is only 54) since we normalized the variables, but that won't change the RMSE.

```{r}
# Get LASSO predictions
lasso_preds <- predict(final_lasso_fit, new_data = test) %>%
  bind_cols(test)

# Compute RMSE
rmse_lasso <- rmse(lasso_preds, truth = Age, estimate = .pred)
rmse_lasso
```

### Question 6: Compare RMSE

We can see that both models have similar RMSE.

```{r}
rmse_lasso
OLS_rmse
```

### Question 7: Why are they same even though we have different coefficient?

As stated, we standardized our variables in the LASSO model, which makes the coefficients look different. However, we use the same variables and interactions, so they are making very similar predictions. Thus, our RMSE is nearly the same.

## Task 6: Logistic Regression

### Question 1: Fit 2 models and Identify Best models

```{r}
#specify control
ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3, classProbs = TRUE, summaryFunction = twoClassSummary)

# Model 1
logit_model1 <- train(
  Heart_Disease ~ MaxHR + Age + Sex + Cholesterol,
  data = train,
  method = "glm",
  family = "binomial",
  trControl = ctrl,
  metric = "ROC"
)

# Model 2
logit_model2 <- train(
  Heart_Disease ~ Age + MaxHR + Cholesterol + RestingBP + Sex + ChestPainType,
  data = train,
  method = "glm",
  family = "binomial",
  trControl = ctrl,
  metric = "ROC"
)

#ID best model
logit_model1$results
logit_model2$results

#provide summary of best model
summary(logit_model2$finalModel) 
```

Model 2 is our best model with an ROC of .848 (vs .771). The sensitivy and specificity standard deviation is also lower among folds for model 2, which indicates consistency. Model 2 also has higher sensitivity and specificity, which I will define in question 3.

### Question 2: Check on Test data with confusion matrix

```{r}
#Use trained model on test data
logit_preds <- predict(logit_model2, newdata = test)
conf_matrix <- confusionMatrix(logit_preds, test$Heart_Disease)
conf_matrix
```

### Question 3: ID Values of Sensitivity etc.

```{r}
conf_matrix$byClass["Sensitivity"]
conf_matrix$byClass["Specificity"]
```

Sensitivity: Proportion of people with heart disease that our model correctly identifies

-   Our Model can correctly identify 76.6% of people with heart disease (23.4% false negative)

Specificity: proportion of people without heart disease that our model correctly identifies

-   Our model can correctly identify 77.8% of people without heart disease (22.2% false positive)
